{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notebook 03: Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ! pip install psycopg2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.2.4\n"
     ]
    }
   ],
   "source": [
    "import keras\n",
    "print(keras.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.12.0\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json, time, re, string, keras, adanet, pickle\n",
    "import pandas as pd\n",
    "import psycopg2 as pg2\n",
    "import numpy as np\n",
    "\n",
    "from numpy import random\n",
    "from psycopg2.extras import RealDictCursor, Json\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Embedding, LSTM\n",
    "from keras.utils.np_utils import to_categorical\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline\n",
    "%run ../assets/sql_cred.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filename_format_log(file_path, \n",
    "                        logfile = '../assets/file_log.txt', \n",
    "                        now = round(time.time()), \n",
    "                        file_description = None): \n",
    "   \n",
    "    try:\n",
    "        ext = re.search('(?<!^)(?<!\\.)\\.(?!\\.)', file_path).start() \n",
    "    except:\n",
    "        raise NameError('Please enter a relative path with a file extension.') \n",
    "    \n",
    "    stamp = re.search('(?<!^)(?<!\\.)[0-z]+_[0-z]+(?=\\.)', file_path).start()\n",
    "    formatted_name = f'{file_path[:stamp]}{now}_{file_path[stamp:]}'  \n",
    "    if not file_description:\n",
    "        file_description = f'Saved at: {time.asctime(time.gmtime(now))}'\n",
    "    with open(logfile, 'a+') as f:\n",
    "        f.write(f'{formatted_name}: {file_description}\\n')\n",
    "    return formatted_name, now, file_description"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def con_cur_to_db(dbname=DBNAME, dict_cur=None):\n",
    "    con = pg2.connect(host=IP_ADDRESS,\n",
    "                  dbname=dbname,\n",
    "                  user=USER,\n",
    "                  password=PASSWORD)\n",
    "    if dict_cur:\n",
    "        cur = con.cursor(cursor_factory=RealDictCursor)\n",
    "    else:\n",
    "        cur = con.cursor()\n",
    "    return con, cur\n",
    "    \n",
    "def execute_query(query, dbname=DBNAME, dict_cur=None, command=False):\n",
    "    con, cur = con_cur_to_db(dbname, dict_cur)\n",
    "    cur.execute(f'{query}')\n",
    "    if not command:\n",
    "        data = cur.fetchall()\n",
    "        con.close()\n",
    "        return data\n",
    "    con.commit() #sends to server\n",
    "    con.close() #closes server connection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "lyric_df = pd.read_csv('../assets/1548873539_clean_lyrics.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>lyrics</th>\n",
       "      <th>clean_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>\\n\\nIf your needle is near\\nNeedle is near\\nYo...</td>\n",
       "      <td>if your needle is near \\n needle is near \\n yo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>\\n\\n[Verse 1]\\nBrown skin girl on the other si...</td>\n",
       "      <td>brown skin girl on the other side of the room ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>\\n\\n[Verse 1]\\nIt's simple, I love it\\nHaving ...</td>\n",
       "      <td>its simple i love it \\n having you near me hav...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>\\n\\n[Intro: Whistling]\\n\\n[Verse 1]\\nA great b...</td>\n",
       "      <td>a great big bang and dinosaurs \\n fiery rainin...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>\\n\\n[Verse 1]\\nIsn't she lovely\\nIsn't she won...</td>\n",
       "      <td>isnt she lovely \\n isnt she wonderful \\n isnt ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              lyrics  \\\n",
       "0  \\n\\nIf your needle is near\\nNeedle is near\\nYo...   \n",
       "1  \\n\\n[Verse 1]\\nBrown skin girl on the other si...   \n",
       "2  \\n\\n[Verse 1]\\nIt's simple, I love it\\nHaving ...   \n",
       "3  \\n\\n[Intro: Whistling]\\n\\n[Verse 1]\\nA great b...   \n",
       "4  \\n\\n[Verse 1]\\nIsn't she lovely\\nIsn't she won...   \n",
       "\n",
       "                                          clean_text  \n",
       "0  if your needle is near \\n needle is near \\n yo...  \n",
       "1  brown skin girl on the other side of the room ...  \n",
       "2  its simple i love it \\n having you near me hav...  \n",
       "3  a great big bang and dinosaurs \\n fiery rainin...  \n",
       "4  isnt she lovely \\n isnt she wonderful \\n isnt ...  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lyric_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "lyric_df = lyric_df.drop(index=[193], axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>lyrics</th>\n",
       "      <th>clean_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>1800</td>\n",
       "      <td>1800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>unique</th>\n",
       "      <td>1800</td>\n",
       "      <td>1800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>top</th>\n",
       "      <td>\\n\\n[Verse 1]\\nSee the stone set in your eyes\\...</td>\n",
       "      <td>ive making time for tunes \\n making tunes a pl...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>freq</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   lyrics  \\\n",
       "count                                                1800   \n",
       "unique                                               1800   \n",
       "top     \\n\\n[Verse 1]\\nSee the stone set in your eyes\\...   \n",
       "freq                                                    1   \n",
       "\n",
       "                                               clean_text  \n",
       "count                                                1800  \n",
       "unique                                               1800  \n",
       "top     ive making time for tunes \\n making tunes a pl...  \n",
       "freq                                                    1  "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lyric_df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.6/site-packages/ipykernel_launcher.py:1: UserWarning: This pattern has match groups. To actually get the groups, use str.extract.\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>lyrics</th>\n",
       "      <th>clean_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>\\n\\n3/3\\nBoy Rex - The Bloodmonths - 8/8\\nJoey...</td>\n",
       "      <td>boy rex  the bloodmonths   \\n joey fatts  ill ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>142</th>\n",
       "      <td>\\n\\n[Verse 1]\\nJenny, Jenny, who can I turn to...</td>\n",
       "      <td>jenny jenny who can i turn to \\n you give me s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>195</th>\n",
       "      <td>\\n\\n[NEW]  1. TWICE - What Is Love?\\n[NEW] 2. ...</td>\n",
       "      <td>twice  what is love \\n   exocbx  blooming day ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>260</th>\n",
       "      <td>\\n\\n[Verse 1]:\\nElectric lights\\nBlow my mind\\...</td>\n",
       "      <td>electric lights \\n blow my mind \\n i feel alri...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>552</th>\n",
       "      <td>\\n\\nBaby, baby yea\\nYou run on my mind yea\\nSa...</td>\n",
       "      <td>baby baby yea \\n you run on my mind yea \\n sam...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>938</th>\n",
       "      <td>\\n\\n[Verse 1]\\nThere are two of us on the run\\...</td>\n",
       "      <td>there are two of us on the run \\n going so fas...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>960</th>\n",
       "      <td>\\n\\n[Verse 1]\\nYou're my baby, my lover, my la...</td>\n",
       "      <td>youre my baby my lover my lady \\n all night yo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1106</th>\n",
       "      <td>\\n\\nORIGINALTracklist1.  Let's Go Crazy\\n2.  T...</td>\n",
       "      <td>originaltracklist  lets go crazy \\n   take me ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1169</th>\n",
       "      <td>\\n\\n[Intro-Live Acoustic]\\nKnow your place amo...</td>\n",
       "      <td>know your place among the dark arms of the woo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1495</th>\n",
       "      <td>\\n\\n[Verse 1]\\nIt was a day\\nJust like any oth...</td>\n",
       "      <td>it was a day \\n just like any other day \\n i w...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1545</th>\n",
       "      <td>\\n\\n[Intro: Too $hort]\\nYou know, it's a hard ...</td>\n",
       "      <td>you know its a hard pill to swallow \\n when th...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1572</th>\n",
       "      <td>\\n\\n[Verse 1]\\nWrapped up, so consumed by\\nAll...</td>\n",
       "      <td>wrapped up so consumed by \\n all this hurt \\n ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1688</th>\n",
       "      <td>\\n\\n[Intro: Britt Nicole]\\nIt feels like a tea...</td>\n",
       "      <td>it feels like a tear in my heart \\n like a par...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1768</th>\n",
       "      <td>\\n\\nहाथ में रात ये बाकी है\\nबात नापाक वफ़ा की ह...</td>\n",
       "      <td>bang like a drum work my body \\n   im incharge...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 lyrics  \\\n",
       "31    \\n\\n3/3\\nBoy Rex - The Bloodmonths - 8/8\\nJoey...   \n",
       "142   \\n\\n[Verse 1]\\nJenny, Jenny, who can I turn to...   \n",
       "195   \\n\\n[NEW]  1. TWICE - What Is Love?\\n[NEW] 2. ...   \n",
       "260   \\n\\n[Verse 1]:\\nElectric lights\\nBlow my mind\\...   \n",
       "552   \\n\\nBaby, baby yea\\nYou run on my mind yea\\nSa...   \n",
       "938   \\n\\n[Verse 1]\\nThere are two of us on the run\\...   \n",
       "960   \\n\\n[Verse 1]\\nYou're my baby, my lover, my la...   \n",
       "1106  \\n\\nORIGINALTracklist1.  Let's Go Crazy\\n2.  T...   \n",
       "1169  \\n\\n[Intro-Live Acoustic]\\nKnow your place amo...   \n",
       "1495  \\n\\n[Verse 1]\\nIt was a day\\nJust like any oth...   \n",
       "1545  \\n\\n[Intro: Too $hort]\\nYou know, it's a hard ...   \n",
       "1572  \\n\\n[Verse 1]\\nWrapped up, so consumed by\\nAll...   \n",
       "1688  \\n\\n[Intro: Britt Nicole]\\nIt feels like a tea...   \n",
       "1768  \\n\\nहाथ में रात ये बाकी है\\nबात नापाक वफ़ा की ह...   \n",
       "\n",
       "                                             clean_text  \n",
       "31    boy rex  the bloodmonths   \\n joey fatts  ill ...  \n",
       "142   jenny jenny who can i turn to \\n you give me s...  \n",
       "195   twice  what is love \\n   exocbx  blooming day ...  \n",
       "260   electric lights \\n blow my mind \\n i feel alri...  \n",
       "552   baby baby yea \\n you run on my mind yea \\n sam...  \n",
       "938   there are two of us on the run \\n going so fas...  \n",
       "960   youre my baby my lover my lady \\n all night yo...  \n",
       "1106  originaltracklist  lets go crazy \\n   take me ...  \n",
       "1169  know your place among the dark arms of the woo...  \n",
       "1495  it was a day \\n just like any other day \\n i w...  \n",
       "1545  you know its a hard pill to swallow \\n when th...  \n",
       "1572  wrapped up so consumed by \\n all this hurt \\n ...  \n",
       "1688  it feels like a tear in my heart \\n like a par...  \n",
       "1768  bang like a drum work my body \\n   im incharge...  "
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lyric_df[lyric_df['clean_text'].str.contains(r'(\\s{6,})')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "lyric_df = lyric_df.drop(index=1768, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_sequence(text, sequence_length = 7, output_length = 4):\n",
    "    \n",
    "    X, y = [], []\n",
    "    \n",
    "    split_text = re.split('(\\n)|(\\[.+\\])|\\s', text)\n",
    "    split_text = list(filter(None, split_text))\n",
    "    split_text = text\n",
    "    \n",
    "    for i in range(len(split_text) - sequence_length):\n",
    "        X.append(split_text[i:i + sequence_length])\n",
    "        y.append(split_text[i + sequence_length:i + sequence_length + output_length])\n",
    "        \n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_samples(X_indexed, y_indexed):\n",
    "    partition = {}\n",
    "    labels = {}\n",
    "\n",
    "    random_seed = 42\n",
    "    rand_ind = random.choice(range(len(X_indexed)), len(X_indexed), replace=False)\n",
    "    \n",
    "    partition['train'] = rand_ind[:int(np.ceil(len(rand_ind)*.8))]\n",
    "    partition['validation'] = rand_ind[-int(np.ceil(len(rand_ind)*.2))+1:]\n",
    "\n",
    "    for i in rand_ind:\n",
    "        labels[i] = y_indexed[i]\n",
    "        \n",
    "    return partition, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_lyrics(df, lyrics_col, seq_len, output_len, save_dir='../assets'):\n",
    "    X = []\n",
    "    y = []\n",
    "    \n",
    "    corpus = []\n",
    "    \n",
    "    print('Processing lyrics...')\n",
    "    for _, track in df[lyrics_col].iterrows():\n",
    "        lyrics = track[0]\n",
    "        lyrics_spaced = re.sub(r'( +)', ' ', lyrics)\n",
    "        lyrics_split = lyrics_spaced.split(' ')\n",
    "        corpus.extend(lyrics_split)\n",
    "                \n",
    "        for i in range(len(lyrics_split) - seq_len):\n",
    "            X.append(np.array(lyrics_split[i:i + seq_len]))\n",
    "            y.extend(np.array(lyrics_split[i + seq_len:i + seq_len + output_len]))\n",
    "            \n",
    "    print('Fitting Tokenizer...')\n",
    "    tokenizer = Tokenizer(oov_token=0)\n",
    "    tokenizer.filters = tokenizer.filters.replace('\\n', '')\n",
    "    tokenizer.fit_on_texts(corpus)\n",
    "\n",
    "    vocab_size = len(tokenizer.word_index) + 1\n",
    "        \n",
    "    print(f'Vocab size = {vocab_size}')\n",
    "#         formatted_name, now, file_description= filename_format_log(f'{save_dir}/tokenizer.pkl')\n",
    "\n",
    "#         with open(formatted_name, 'wb+') as f:\n",
    "#             pickle.dump(tokenizer, f)\n",
    "#         print(f'Tokenizer saved to {formatted_name}.')          \n",
    "\n",
    "    print('Indexing sequences...')\n",
    "    X_indexed = [[tokenizer.texts_to_sequences([word])[0] for word in row] for row in X]\n",
    "    y_indexed = [tokenizer.texts_to_sequences([word])[0] for word in y]\n",
    "    \n",
    "    print('Partitioning and converting to labels...')\n",
    "\n",
    "    partition, labels = generate_samples(X_indexed, y_indexed)\n",
    "    \n",
    "    np.save(f'{save_dir}/data.npy', partition)\n",
    "#     X_reshape = np.reshape(X_indexed, (len(X_indexed), seq_len, 1))\n",
    " \n",
    "#     y_cat = to_categorical(y_indexed)\n",
    "    \n",
    "    print('Lyrics successfully tokenized, sequenced, and indexed.') \n",
    "    \n",
    "    return partition, labels, vocab_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing lyrics...\n",
      "Fitting Tokenizer...\n",
      "Vocab size = 13470\n",
      "Indexing sequences...\n",
      "Partitioning and converting to labels...\n",
      "Lyrics successfully tokenized, sequenced, and indexed.\n"
     ]
    }
   ],
   "source": [
    "partition, labels, vocab_size = tokenize_lyrics(df=lyric_df,\n",
    "                                   lyrics_col=['clean_text'],\n",
    "                                   seq_len=4,\n",
    "                                   output_len=1,\n",
    "                                   save_dir='../assets'\n",
    "                                  )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'train': array([356649, 109240,   2957, ...,  87503, 436021,  78552]),\n",
       " 'validation': array([435192, 493343, 395344, ..., 133438, 191157, 320662])}"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "partition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(partition)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "numpy.int64"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(partition['train'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "not_int = []\n",
    "for i, part in enumerate(partition['train']):\n",
    "    if not np.issubdtype(partition['train'][0],np.int64):\n",
    "        not_int.append(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "not_int"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = X_indexed[103231]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "576641"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(X_indexed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[2], [4], [57], [127]]"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[101]"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels[363226]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array({'train': array([471140, 539156, 183285, ..., 148349,  67356,  86803]), 'validation': array([449682, 181791, 197150, ..., 369885, 349343,  46369])}, dtype=object)"
      ]
     },
     "execution_count": 146,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.load('../assets/data.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "461313"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "partition['train'].shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataGenerator(keras.utils.Sequence):\n",
    "    'Generates data for Keras'\n",
    "    def __init__(self, list_IDs, labels, batch_size=5000, dim=(5000,1,4), n_channels=1,\n",
    "                 n_classes=10, shuffle=True):\n",
    "        'Initialization'\n",
    "        self.dim = dim\n",
    "        self.batch_size = batch_size\n",
    "        self.labels = labels\n",
    "        self.list_IDs = list_IDs\n",
    "        self.n_channels = n_channels\n",
    "        self.n_classes = n_classes\n",
    "        self.shuffle = shuffle\n",
    "        self.on_epoch_end()\n",
    "\n",
    "    def __len__(self):\n",
    "        'Denotes the number of batches per epoch'\n",
    "        return int(np.floor(len(self.list_IDs) / self.batch_size))\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        'Generate one batch of data'\n",
    "        # Generate indexes of the batch\n",
    "        indexes = self.indexes[index*self.batch_size:(index+1)*self.batch_size]\n",
    "\n",
    "        # Find list of IDs\n",
    "        list_IDs_temp = [self.list_IDs[k] for k in indexes]\n",
    "\n",
    "        # Generate data\n",
    "        X, y = self.__data_generation(list_IDs_temp)\n",
    "\n",
    "        return X, y\n",
    "\n",
    "    def on_epoch_end(self):\n",
    "        'Updates indexes after each epoch'\n",
    "        self.indexes = np.arange(len(self.list_IDs))\n",
    "        if self.shuffle == True:\n",
    "            np.random.shuffle(self.indexes)\n",
    "\n",
    "    def __data_generation(self, list_IDs_temp):\n",
    "        'Generates data containing batch_size samples' # X : (n_samples, *dim, n_channels)\n",
    "        # Initialization\n",
    "        X = np.empty((self.batch_size, *self.dim, self.n_channels))\n",
    "        y = np.empty((self.batch_size), dtype=int)\n",
    "\n",
    "        # Generate data\n",
    "        for i, ID in enumerate(list_IDs_temp):\n",
    "            # Store sample\n",
    "            X[i] = self.list_IDs[ID]\n",
    "\n",
    "            # Store class\n",
    "            y[i] = self.labels[ID]\n",
    "\n",
    "        return X, keras.utils.to_categorical(y, num_classes=self.n_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters\n",
    "params = {'dim': (5000,1,4),\n",
    "          'batch_size': 5000,\n",
    "          'n_classes': vocab_size,\n",
    "          'n_channels': 1,\n",
    "          'shuffle': True\n",
    "         }\n",
    "\n",
    "# Datasets\n",
    "partition = partition\n",
    "labels = labels\n",
    "\n",
    "# Generators\n",
    "training_generator = DataGenerator(partition['train'], labels, **params)\n",
    "validation_generator = DataGenerator(partition['validation'], labels, **params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile model\n",
    "model = Sequential()\n",
    "model.add(LSTM(32, input_shape=(4, 1)))\n",
    "model.add(Dense(100, activation='relu'))\n",
    "model.add(Dense(vocab_size, activation='softmax'))\n",
    "\n",
    "model.compile(\n",
    "    loss = 'categorical_crossentropy',\n",
    "    optimizer = 'adam',\n",
    "    metrics = ['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "setting an array element with a sequence.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRemoteTraceback\u001b[0m                           Traceback (most recent call last)",
      "\u001b[0;31mRemoteTraceback\u001b[0m: \n\"\"\"\nTraceback (most recent call last):\n  File \"/opt/conda/lib/python3.6/multiprocessing/pool.py\", line 119, in worker\n    result = (True, func(*args, **kwds))\n  File \"/opt/conda/lib/python3.6/site-packages/keras/utils/data_utils.py\", line 401, in get_index\n    return _SHARED_SEQUENCES[uid][i]\n  File \"<ipython-input-181-c4fa64ee09e5>\", line 28, in __getitem__\n    X, y = self.__data_generation(list_IDs_temp)\n  File \"<ipython-input-181-c4fa64ee09e5>\", line 50, in __data_generation\n    y[i] = self.labels[ID]\nValueError: setting an array element with a sequence.\n\"\"\"",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-184-65a1ca5e65ad>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m                     \u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m                     \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m                     \u001b[0mworkers\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m6\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m                    )\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.6/site-packages/keras/legacy/interfaces.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     89\u001b[0m                 warnings.warn('Update your `' + object_name + '` call to the ' +\n\u001b[1;32m     90\u001b[0m                               'Keras 2 API: ' + signature, stacklevel=2)\n\u001b[0;32m---> 91\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     92\u001b[0m         \u001b[0mwrapper\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_original_function\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit_generator\u001b[0;34m(self, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)\u001b[0m\n\u001b[1;32m   1416\u001b[0m             \u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1417\u001b[0m             \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mshuffle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1418\u001b[0;31m             initial_epoch=initial_epoch)\n\u001b[0m\u001b[1;32m   1419\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1420\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0minterfaces\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlegacy_generator_methods_support\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.6/site-packages/keras/engine/training_generator.py\u001b[0m in \u001b[0;36mfit_generator\u001b[0;34m(model, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)\u001b[0m\n\u001b[1;32m    179\u001b[0m             \u001b[0mbatch_index\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    180\u001b[0m             \u001b[0;32mwhile\u001b[0m \u001b[0msteps_done\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 181\u001b[0;31m                 \u001b[0mgenerator_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput_generator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    182\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    183\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgenerator_output\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'__len__'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.6/site-packages/keras/utils/data_utils.py\u001b[0m in \u001b[0;36mget\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    599\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    600\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 601\u001b[0;31m             \u001b[0msix\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreraise\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexc_info\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    602\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    603\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.6/site-packages/six.py\u001b[0m in \u001b[0;36mreraise\u001b[0;34m(tp, value, tb)\u001b[0m\n\u001b[1;32m    691\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mtb\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    692\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 693\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    694\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    695\u001b[0m             \u001b[0mvalue\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.6/site-packages/keras/utils/data_utils.py\u001b[0m in \u001b[0;36mget\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    593\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    594\u001b[0m             \u001b[0;32mwhile\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_running\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 595\u001b[0;31m                 \u001b[0minputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mqueue\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mblock\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    596\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mqueue\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtask_done\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    597\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0minputs\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.6/multiprocessing/pool.py\u001b[0m in \u001b[0;36mget\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    642\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_value\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    643\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 644\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_value\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    645\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    646\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_set\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: setting an array element with a sequence."
     ]
    }
   ],
   "source": [
    "# Train model on dataset\n",
    "model.fit_generator(generator=training_generator,\n",
    "                    validation_data=validation_generator,\n",
    "                    use_multiprocessing=True,\n",
    "                    verbose=1,\n",
    "                    workers=6\n",
    "                   )\n",
    "\n",
    "# Save model\n",
    "# formatted_name, now, file_description= filename_format_log('../assets/LSTM_Model.pkl')\n",
    "\n",
    "# with open(formatted_name, 'wb+') as f:\n",
    "#     pickle.dump(model, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print model summary\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model.fit (\n",
    "    X_train, y_train,\n",
    "    epochs = 150,\n",
    "    batch_size = 2500,\n",
    "    verbose = 1,\n",
    "    \n",
    ")\n",
    "\n",
    "# formatted_name, now, file_description= filename_format_log('../assets/LSTM_Model.pkl')\n",
    "\n",
    "# with open(formatted_name, 'wb+') as f:\n",
    "#     pickle.dump(model, f)\n",
    "\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(ncols=2, nrows=1, figsize=(18,6))\n",
    "\n",
    "ax[0].plot(history.history['loss'])\n",
    "ax[0].set_title(\"Loss\", fontsize=15);\n",
    "ax[0].set_xlabel(\"epochs\",fontsize=15);\n",
    "\n",
    "ax[1].plot(history.history['acc'])\n",
    "ax[1].set_title(\"Accuracy\",fontsize=15);\n",
    "ax[1].set_xlabel(\"epochs\",fontsize=15);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_lyrics(seed, model=model, seq_len=4, song_len=50):\n",
    "    seed_clean = seed.lower().split(' ')\n",
    "    doc = []\n",
    "\n",
    "    while len(doc) < song_len:\n",
    "        text = [seed_clean]\n",
    "        sequence = [tokenizer.texts_to_sequences([word])[0] for word in text]\n",
    "        pad_sequence = pad_sequences(sequence, maxlen=seq_len, truncating='pre')\n",
    "        sequence_reshape = np.reshape(pad_sequence, (len(test_indexed), 4, 1))\n",
    "\n",
    "        yhat = model.predict_classes(sequence_reshape, verbose=0)\n",
    "\n",
    "        for word, index in tokenizer.word_index.items():\n",
    "            if index == yhat:\n",
    "                seed_clean.append(word)\n",
    "                doc.append(word)\n",
    "\n",
    "    return ' '.join(doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lyrics = generate_lyrics('needles are for lovers', song_len=150)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lyrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# formatted_name, now, file_description= filename_format_log(file_path = '../assets/lyric_X74.pkl')\n",
    "\n",
    "# with open(formatted_name, 'wb+') as f:\n",
    "#     pickle.dump(X, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# formatted_name, now, file_description= filename_format_log(file_path = '../assets/lyric_y74.pkl')\n",
    "\n",
    "# with open(formatted_name, 'wb+') as f:\n",
    "#     pickle.dump(y, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "lyric_df['cleaner_text'] = lyric_df['clean_text'].map(lambda x: re.sub(r'( +)', ' ', x).split(' '))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[corpus.add(word) for song in lyric_df['clean_text'].map(lambda x: re.sub(r'( +)', ' ', x).split(' ')).values for word in song];"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "seq_len, output_len = 4, 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "def sequencer(lyrics_split, seq_len=4, output_len=1):\n",
    "    X, y = [], []\n",
    "    for i in range(len(lyrics_split) - seq_len):\n",
    "        X.append(lyrics_split[i:i + seq_len])\n",
    "        y.extend(lyrics_split[i + seq_len:i + seq_len + output_len])\n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "foo = pd.DataFrame([1,2,3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "result = lyric_df.cleaner_text.map(sequencer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "result[0][1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "lyrics_split[i + 4:i + 4 + 1]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
