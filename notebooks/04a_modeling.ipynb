{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notebook 04: Modeling - Keras Embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Contents:\n",
    "1. [Summary](#section1)\n",
    "2. [Dropping or Amending Lyrics](#section2)\n",
    "3. [Cleaning Lyrics](#section3)\n",
    "4. [Song Structure Analysis](#section4)\n",
    "5. [Plotting](#section5)\n",
    "6. [Lyric Structure Analysis](#section6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Summary\n",
    "In this notebook I will build out a LSTM model in Keras that will take a sequence of 4 tokenized words from a lyric and predict the 5th. The input layer will be a Keras Embedding layer that will learn the relationships between words as the model trains, adding a layer of cohesion and an aspect of memory to the predictions. THis will alos help densify the input layer from one-hot categorical encodings."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Importing the necessary libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json, time, re, string, keras, pickle, adanet\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "from numpy import random\n",
    "\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Embedding, LSTM\n",
    "from keras.utils.np_utils import to_categorical\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline\n",
    "%run ../assets/sql_cred.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since this is being run on an AWS EC2 GPU instance, I am verifying that the GPU is being recognized by the kernal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/device:GPU:0\n"
     ]
    }
   ],
   "source": [
    "print(tf.test.gpu_device_name())\n",
    "\n",
    "config = tf.ConfigProto()\n",
    "config.gpu_options.allow_growth = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reading in the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_reshape = np.load('../assets/1549351379_LSTM315_Xreshape.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_cat_coo = sparse.load_npz('../assets/1549351379_LSTM315_ycat.npz')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_cat = y_cat_coo.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../assets/1549351379_LSTM315_53tokenizer.pkl', 'rb') as f:\n",
    "    tokenizer = pickle.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Saving vocab_size to the local namespace so it can be referenced as the output shape for the final layer of my model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = len(tokenizer.word_index) + 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building the Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LSTM models have shown great performance for NLP processes such as text generation. They build on the framework of recurrent neural networks (RNN) by adding an additional cell state that allows the model to tune how data persists over time. This helps address one of the shortcomings of more simple RNNs - the vanishing gradient problem where data occurring earlier in the dataset becomes less likely to be considered when making predictions. This persistence is balanced by a 'forget gate' which allows data to be selectively dropped from memory. As the model trains on the input data these parameters are tuned to optimize predictions based on a given loss function. \n",
    "\n",
    "Because I want the model to understand the relationship between words, not just from a context but also from content, I am setting the input layer as an Embedding layer. This will be initialized with random weights that will be optimized as the model fits. This hope is that adding this additional layer of meaning will support better predictictions with user generated seed inputs. Since the vocabulary is relatively large, I'm starting with a relatively modest densification of the data with a 3000 feature embedding space.\n",
    "\n",
    "The embedding layer will then be passed to the first LSTM layer. This layer will consist of 300 nodes and allow sequences to be passed to the next LSTM layer of 150 nodes. This aspectct is important as it relates to the core functional difference between LSTM and traditional RNN layers. Without the ability to pass sequences downstream the first layer would effectctively function like a leaky RNN. \n",
    "\n",
    "The second LSTM layer will pass the data to a dense layer with 100 nodes and a relu activation function. The idea here is that the model is wide enough in the early layers to pick out important features without overtraining and just returning an existing lyric or loop. By starting with 3000 features and narrowing as data moves downstream, there will be a funnel effect resulting in more generalizable predictions that account for a broader understanding of relationship in the data.\n",
    "\n",
    "Finally, the output layer will be a softmax activated dense layer with the same shape as the input vocabulary. This will allow for predictions tied back to the initial corpus. The model will be optimized with ADAM, an adaptive gradient descent function with randomized initialization points which allows it to be faster and generally more effective than other optimization algorithms. Finally, the model will be assessed on accuracy based on the ground truth of actual next words in the track."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile model\n",
    "model = Sequential()\n",
    "model.add(Embedding(vocab_size, 3000, mask_zero=True))\n",
    "model.add(LSTM(300, return_sequences=True))\n",
    "model.add(LSTM(150))\n",
    "model.add(Dense(100, activation='relu'))\n",
    "model.add(Dense(y_cat.shape[1], activation='softmax'))\n",
    "\n",
    "model.compile(\n",
    "    loss = 'categorical_crossentropy',\n",
    "    optimizer = 'adam',\n",
    "    metrics = ['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model will be run on GPU with a relatively large batch size of 10000 sequences over 150 epochs. This is somewhat modest considering the model is being fit without the foundation of pre-trained word vectors. Considering the size of the dataset, nearly 600000 sequences, this will allow for a reasonable starting point to assess model architecture before moving toward potentially more complex or time consuming systems."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/150\n",
      "576641/576641 [==============================] - 91s 158us/step - loss: 5.9092 - acc: 0.1141\n",
      "Epoch 2/150\n",
      "576641/576641 [==============================] - 91s 158us/step - loss: 5.8179 - acc: 0.1237\n",
      "Epoch 3/150\n",
      "576641/576641 [==============================] - 91s 157us/step - loss: 5.8075 - acc: 0.1237\n",
      "Epoch 4/150\n",
      "576641/576641 [==============================] - 91s 158us/step - loss: 5.7988 - acc: 0.1237\n",
      "Epoch 5/150\n",
      "576641/576641 [==============================] - 91s 158us/step - loss: 5.7914 - acc: 0.1237\n",
      "Epoch 6/150\n",
      "576641/576641 [==============================] - 91s 158us/step - loss: 5.7862 - acc: 0.1237\n",
      "Epoch 7/150\n",
      "576641/576641 [==============================] - 91s 158us/step - loss: 5.7787 - acc: 0.1237\n",
      "Epoch 8/150\n",
      "576641/576641 [==============================] - 91s 158us/step - loss: 5.7686 - acc: 0.1237\n",
      "Epoch 9/150\n",
      "576641/576641 [==============================] - 91s 158us/step - loss: 5.7573 - acc: 0.1237\n",
      "Epoch 10/150\n",
      "576641/576641 [==============================] - 92s 159us/step - loss: 5.6589 - acc: 0.1260\n",
      "Epoch 11/150\n",
      "576641/576641 [==============================] - 92s 159us/step - loss: 5.3783 - acc: 0.1450\n",
      "Epoch 12/150\n",
      "576641/576641 [==============================] - 92s 159us/step - loss: 5.1438 - acc: 0.1635\n",
      "Epoch 13/150\n",
      "576641/576641 [==============================] - 92s 159us/step - loss: 4.9518 - acc: 0.1806\n",
      "Epoch 14/150\n",
      "576641/576641 [==============================] - 92s 159us/step - loss: 4.7921 - acc: 0.1954\n",
      "Epoch 15/150\n",
      "576641/576641 [==============================] - 92s 159us/step - loss: 4.6591 - acc: 0.2072\n",
      "Epoch 16/150\n",
      "576641/576641 [==============================] - 92s 159us/step - loss: 4.5395 - acc: 0.2187\n",
      "Epoch 17/150\n",
      "576641/576641 [==============================] - 92s 159us/step - loss: 4.4299 - acc: 0.2298\n",
      "Epoch 18/150\n",
      "576641/576641 [==============================] - 92s 159us/step - loss: 4.3277 - acc: 0.2397\n",
      "Epoch 19/150\n",
      "576641/576641 [==============================] - 92s 159us/step - loss: 4.2279 - acc: 0.2507\n",
      "Epoch 20/150\n",
      "576641/576641 [==============================] - 92s 159us/step - loss: 4.1294 - acc: 0.2620\n",
      "Epoch 21/150\n",
      "576641/576641 [==============================] - 92s 159us/step - loss: 4.0341 - acc: 0.2732\n",
      "Epoch 22/150\n",
      "576641/576641 [==============================] - 92s 159us/step - loss: 3.9391 - acc: 0.2854\n",
      "Epoch 23/150\n",
      "576641/576641 [==============================] - 92s 159us/step - loss: 3.8425 - acc: 0.2984\n",
      "Epoch 24/150\n",
      "576641/576641 [==============================] - 92s 159us/step - loss: 3.7515 - acc: 0.3108\n",
      "Epoch 25/150\n",
      "576641/576641 [==============================] - 92s 159us/step - loss: 3.6603 - acc: 0.3241\n",
      "Epoch 26/150\n",
      "576641/576641 [==============================] - 92s 159us/step - loss: 3.5668 - acc: 0.3380\n",
      "Epoch 27/150\n",
      "576641/576641 [==============================] - 92s 159us/step - loss: 3.4782 - acc: 0.3517\n",
      "Epoch 28/150\n",
      "576641/576641 [==============================] - 92s 159us/step - loss: 3.3910 - acc: 0.3649\n",
      "Epoch 29/150\n",
      "576641/576641 [==============================] - 92s 159us/step - loss: 3.3063 - acc: 0.3787\n",
      "Epoch 30/150\n",
      "576641/576641 [==============================] - 92s 159us/step - loss: 3.2237 - acc: 0.3924\n",
      "Epoch 31/150\n",
      "576641/576641 [==============================] - 92s 159us/step - loss: 3.1427 - acc: 0.4049\n",
      "Epoch 32/150\n",
      "576641/576641 [==============================] - 92s 159us/step - loss: 3.0637 - acc: 0.4181\n",
      "Epoch 33/150\n",
      "576641/576641 [==============================] - 92s 159us/step - loss: 2.9926 - acc: 0.4299\n",
      "Epoch 34/150\n",
      "576641/576641 [==============================] - 92s 159us/step - loss: 2.9231 - acc: 0.4418\n",
      "Epoch 35/150\n",
      "576641/576641 [==============================] - 92s 159us/step - loss: 2.8495 - acc: 0.4537\n",
      "Epoch 36/150\n",
      "576641/576641 [==============================] - 92s 159us/step - loss: 2.7860 - acc: 0.4644\n",
      "Epoch 37/150\n",
      "576641/576641 [==============================] - 92s 159us/step - loss: 2.7202 - acc: 0.4751\n",
      "Epoch 38/150\n",
      "576641/576641 [==============================] - 92s 159us/step - loss: 2.6591 - acc: 0.4857\n",
      "Epoch 39/150\n",
      "576641/576641 [==============================] - 92s 159us/step - loss: 2.6013 - acc: 0.4953\n",
      "Epoch 40/150\n",
      "576641/576641 [==============================] - 92s 159us/step - loss: 2.5473 - acc: 0.5047\n",
      "Epoch 41/150\n",
      "576641/576641 [==============================] - 92s 159us/step - loss: 2.4930 - acc: 0.5137\n",
      "Epoch 42/150\n",
      "576641/576641 [==============================] - 92s 159us/step - loss: 2.4409 - acc: 0.5222\n",
      "Epoch 43/150\n",
      "576641/576641 [==============================] - 92s 159us/step - loss: 2.3929 - acc: 0.5307\n",
      "Epoch 44/150\n",
      "576641/576641 [==============================] - 92s 159us/step - loss: 2.3429 - acc: 0.5394\n",
      "Epoch 45/150\n",
      "576641/576641 [==============================] - 92s 159us/step - loss: 2.2991 - acc: 0.5463\n",
      "Epoch 46/150\n",
      "576641/576641 [==============================] - 92s 159us/step - loss: 2.2576 - acc: 0.5540\n",
      "Epoch 47/150\n",
      "576641/576641 [==============================] - 92s 159us/step - loss: 2.2146 - acc: 0.5614\n",
      "Epoch 48/150\n",
      "576641/576641 [==============================] - 92s 159us/step - loss: 2.1700 - acc: 0.5686\n",
      "Epoch 49/150\n",
      "576641/576641 [==============================] - 92s 159us/step - loss: 2.1352 - acc: 0.5742\n",
      "Epoch 50/150\n",
      "576641/576641 [==============================] - 92s 159us/step - loss: 2.0967 - acc: 0.5811\n",
      "Epoch 51/150\n",
      "576641/576641 [==============================] - 92s 159us/step - loss: 2.0593 - acc: 0.5872\n",
      "Epoch 52/150\n",
      "576641/576641 [==============================] - 92s 159us/step - loss: 2.0240 - acc: 0.5941\n",
      "Epoch 53/150\n",
      "576641/576641 [==============================] - 92s 159us/step - loss: 1.9889 - acc: 0.6000\n",
      "Epoch 54/150\n",
      "576641/576641 [==============================] - 92s 159us/step - loss: 1.9556 - acc: 0.6055\n",
      "Epoch 55/150\n",
      "576641/576641 [==============================] - 92s 159us/step - loss: 1.9244 - acc: 0.6109\n",
      "Epoch 56/150\n",
      "576641/576641 [==============================] - 92s 159us/step - loss: 1.8941 - acc: 0.6171\n",
      "Epoch 57/150\n",
      "576641/576641 [==============================] - 92s 159us/step - loss: 1.8645 - acc: 0.6218\n",
      "Epoch 58/150\n",
      "576641/576641 [==============================] - 92s 159us/step - loss: 1.8336 - acc: 0.6274\n",
      "Epoch 59/150\n",
      "576641/576641 [==============================] - 92s 159us/step - loss: 1.8078 - acc: 0.6315\n",
      "Epoch 60/150\n",
      "576641/576641 [==============================] - 92s 159us/step - loss: 1.7769 - acc: 0.6372\n",
      "Epoch 61/150\n",
      "576641/576641 [==============================] - 92s 159us/step - loss: 1.7546 - acc: 0.6406\n",
      "Epoch 62/150\n",
      "576641/576641 [==============================] - 92s 159us/step - loss: 1.7274 - acc: 0.6459\n",
      "Epoch 63/150\n",
      "576641/576641 [==============================] - 92s 159us/step - loss: 1.7001 - acc: 0.6506\n",
      "Epoch 64/150\n",
      "576641/576641 [==============================] - 92s 160us/step - loss: 1.6752 - acc: 0.6549\n",
      "Epoch 65/150\n",
      "576641/576641 [==============================] - 92s 159us/step - loss: 1.6538 - acc: 0.6590\n",
      "Epoch 66/150\n",
      "576641/576641 [==============================] - 92s 159us/step - loss: 1.6311 - acc: 0.6625\n",
      "Epoch 67/150\n",
      "576641/576641 [==============================] - 92s 159us/step - loss: 1.6099 - acc: 0.6661\n",
      "Epoch 68/150\n",
      "576641/576641 [==============================] - 92s 159us/step - loss: 1.5884 - acc: 0.6698\n",
      "Epoch 69/150\n",
      "576641/576641 [==============================] - 92s 159us/step - loss: 1.5681 - acc: 0.6733\n",
      "Epoch 70/150\n",
      "576641/576641 [==============================] - 92s 159us/step - loss: 1.5443 - acc: 0.6784\n",
      "Epoch 71/150\n",
      "576641/576641 [==============================] - 92s 159us/step - loss: 1.5272 - acc: 0.6808\n",
      "Epoch 72/150\n",
      "576641/576641 [==============================] - 92s 159us/step - loss: 1.5071 - acc: 0.6847\n",
      "Epoch 73/150\n",
      "576641/576641 [==============================] - 92s 159us/step - loss: 1.4920 - acc: 0.6870\n",
      "Epoch 74/150\n",
      "576641/576641 [==============================] - 92s 159us/step - loss: 1.4709 - acc: 0.6912\n",
      "Epoch 75/150\n",
      "576641/576641 [==============================] - 92s 159us/step - loss: 1.4521 - acc: 0.6943\n",
      "Epoch 76/150\n",
      "576641/576641 [==============================] - 92s 159us/step - loss: 1.4373 - acc: 0.6965\n",
      "Epoch 77/150\n",
      "576641/576641 [==============================] - 92s 159us/step - loss: 1.4206 - acc: 0.6993\n",
      "Epoch 78/150\n",
      "576641/576641 [==============================] - 92s 159us/step - loss: 1.4003 - acc: 0.7033\n",
      "Epoch 79/150\n",
      "576641/576641 [==============================] - 92s 159us/step - loss: 1.3849 - acc: 0.7065\n",
      "Epoch 80/150\n",
      "576641/576641 [==============================] - 92s 159us/step - loss: 1.3709 - acc: 0.7080\n",
      "Epoch 81/150\n",
      "576641/576641 [==============================] - 92s 159us/step - loss: 1.3530 - acc: 0.7120\n",
      "Epoch 82/150\n",
      "576641/576641 [==============================] - 92s 159us/step - loss: 1.3388 - acc: 0.7144\n",
      "Epoch 83/150\n",
      "576641/576641 [==============================] - 92s 159us/step - loss: 1.3250 - acc: 0.7167\n",
      "Epoch 84/150\n",
      "576641/576641 [==============================] - 92s 159us/step - loss: 1.3108 - acc: 0.7193\n",
      "Epoch 85/150\n",
      "576641/576641 [==============================] - 92s 159us/step - loss: 1.2954 - acc: 0.7217\n",
      "Epoch 86/150\n",
      "576641/576641 [==============================] - 92s 159us/step - loss: 1.2802 - acc: 0.7249\n",
      "Epoch 87/150\n",
      "576641/576641 [==============================] - 92s 159us/step - loss: 1.2672 - acc: 0.7273\n",
      "Epoch 88/150\n",
      "576641/576641 [==============================] - 92s 159us/step - loss: 1.2542 - acc: 0.7294\n",
      "Epoch 89/150\n",
      "576641/576641 [==============================] - 92s 159us/step - loss: 1.2429 - acc: 0.7314\n",
      "Epoch 90/150\n",
      "576641/576641 [==============================] - 92s 159us/step - loss: 1.2278 - acc: 0.7345\n",
      "Epoch 91/150\n",
      "576641/576641 [==============================] - 92s 159us/step - loss: 1.2172 - acc: 0.7361\n",
      "Epoch 92/150\n",
      "576641/576641 [==============================] - 92s 159us/step - loss: 1.2057 - acc: 0.7383\n",
      "Epoch 93/150\n",
      "576641/576641 [==============================] - 92s 159us/step - loss: 1.1944 - acc: 0.7400\n",
      "Epoch 94/150\n",
      "576641/576641 [==============================] - 92s 160us/step - loss: 1.1847 - acc: 0.7417\n",
      "Epoch 95/150\n",
      "576641/576641 [==============================] - 92s 159us/step - loss: 1.1717 - acc: 0.7439\n",
      "Epoch 96/150\n",
      "576641/576641 [==============================] - 92s 159us/step - loss: 1.1619 - acc: 0.7461\n",
      "Epoch 97/150\n",
      "576641/576641 [==============================] - 92s 159us/step - loss: 1.1495 - acc: 0.7486\n",
      "Epoch 98/150\n",
      "576641/576641 [==============================] - 92s 159us/step - loss: 1.1398 - acc: 0.7495\n",
      "Epoch 99/150\n",
      "576641/576641 [==============================] - 92s 159us/step - loss: 1.1281 - acc: 0.7524\n",
      "Epoch 100/150\n",
      "576641/576641 [==============================] - 92s 160us/step - loss: 1.1179 - acc: 0.7540\n",
      "Epoch 101/150\n",
      "576641/576641 [==============================] - 92s 159us/step - loss: 1.1067 - acc: 0.7555\n",
      "Epoch 102/150\n",
      "576641/576641 [==============================] - 92s 159us/step - loss: 1.1012 - acc: 0.7562\n",
      "Epoch 103/150\n",
      "576641/576641 [==============================] - 92s 159us/step - loss: 1.0905 - acc: 0.7585\n",
      "Epoch 104/150\n",
      "576641/576641 [==============================] - 92s 159us/step - loss: 1.0809 - acc: 0.7604\n",
      "Epoch 105/150\n",
      "576641/576641 [==============================] - 92s 159us/step - loss: 1.0751 - acc: 0.7614\n",
      "Epoch 106/150\n",
      "576641/576641 [==============================] - 92s 159us/step - loss: 1.0662 - acc: 0.7623\n",
      "Epoch 107/150\n",
      "576641/576641 [==============================] - 92s 159us/step - loss: 1.0553 - acc: 0.7645\n",
      "Epoch 108/150\n",
      "576641/576641 [==============================] - 92s 159us/step - loss: 1.0437 - acc: 0.7669\n",
      "Epoch 109/150\n",
      "576641/576641 [==============================] - 92s 159us/step - loss: 1.0389 - acc: 0.7676\n",
      "Epoch 110/150\n",
      "576641/576641 [==============================] - 92s 159us/step - loss: 1.0296 - acc: 0.7692\n",
      "Epoch 111/150\n",
      "576641/576641 [==============================] - 92s 159us/step - loss: 1.0197 - acc: 0.7713\n",
      "Epoch 112/150\n",
      "576641/576641 [==============================] - 92s 159us/step - loss: 1.0140 - acc: 0.7724\n",
      "Epoch 113/150\n",
      "576641/576641 [==============================] - 92s 159us/step - loss: 1.0061 - acc: 0.7733\n",
      "Epoch 114/150\n",
      "576641/576641 [==============================] - 92s 159us/step - loss: 0.9961 - acc: 0.7753\n",
      "Epoch 115/150\n",
      "576641/576641 [==============================] - 92s 159us/step - loss: 0.9903 - acc: 0.7767\n",
      "Epoch 116/150\n",
      "576641/576641 [==============================] - 92s 160us/step - loss: 0.9812 - acc: 0.7776\n",
      "Epoch 117/150\n",
      "576641/576641 [==============================] - 92s 159us/step - loss: 0.9749 - acc: 0.7793\n",
      "Epoch 118/150\n",
      "576641/576641 [==============================] - 92s 159us/step - loss: 0.9658 - acc: 0.7808\n",
      "Epoch 119/150\n",
      "576641/576641 [==============================] - 92s 159us/step - loss: 0.9582 - acc: 0.7819\n",
      "Epoch 120/150\n",
      "576641/576641 [==============================] - 92s 160us/step - loss: 0.9536 - acc: 0.7826\n",
      "Epoch 121/150\n",
      "576641/576641 [==============================] - 92s 160us/step - loss: 0.9474 - acc: 0.7842\n",
      "Epoch 122/150\n",
      "576641/576641 [==============================] - 92s 160us/step - loss: 0.9391 - acc: 0.7851\n",
      "Epoch 123/150\n",
      "576641/576641 [==============================] - 92s 160us/step - loss: 0.9335 - acc: 0.7865\n",
      "Epoch 124/150\n",
      "576641/576641 [==============================] - 92s 159us/step - loss: 0.9263 - acc: 0.7876\n",
      "Epoch 125/150\n",
      "576641/576641 [==============================] - 92s 159us/step - loss: 0.9205 - acc: 0.7886\n",
      "Epoch 126/150\n",
      "576641/576641 [==============================] - 92s 159us/step - loss: 0.9132 - acc: 0.7899\n",
      "Epoch 127/150\n",
      "576641/576641 [==============================] - 92s 160us/step - loss: 0.9086 - acc: 0.7908\n",
      "Epoch 128/150\n",
      "576641/576641 [==============================] - 92s 160us/step - loss: 0.9021 - acc: 0.7924\n",
      "Epoch 129/150\n",
      "576641/576641 [==============================] - 92s 159us/step - loss: 0.8981 - acc: 0.7927\n",
      "Epoch 130/150\n",
      "576641/576641 [==============================] - 92s 160us/step - loss: 0.8908 - acc: 0.7939\n",
      "Epoch 131/150\n",
      "576641/576641 [==============================] - 92s 159us/step - loss: 0.8872 - acc: 0.7948\n",
      "Epoch 132/150\n",
      "576641/576641 [==============================] - 92s 159us/step - loss: 0.8799 - acc: 0.7957\n",
      "Epoch 133/150\n",
      "576641/576641 [==============================] - 92s 160us/step - loss: 0.8748 - acc: 0.7966\n",
      "Epoch 134/150\n",
      "576641/576641 [==============================] - 92s 159us/step - loss: 0.8675 - acc: 0.7981\n",
      "Epoch 135/150\n",
      "576641/576641 [==============================] - 92s 159us/step - loss: 0.8633 - acc: 0.7987\n",
      "Epoch 136/150\n",
      "576641/576641 [==============================] - 92s 160us/step - loss: 0.8593 - acc: 0.7992\n",
      "Epoch 137/150\n",
      "576641/576641 [==============================] - 92s 160us/step - loss: 0.8553 - acc: 0.8001\n",
      "Epoch 138/150\n",
      "576641/576641 [==============================] - 92s 160us/step - loss: 0.8504 - acc: 0.8011\n",
      "Epoch 139/150\n",
      "576641/576641 [==============================] - 92s 159us/step - loss: 0.8440 - acc: 0.8021\n",
      "Epoch 140/150\n",
      "576641/576641 [==============================] - 92s 160us/step - loss: 0.8376 - acc: 0.8033\n",
      "Epoch 141/150\n",
      "576641/576641 [==============================] - 92s 160us/step - loss: 0.8335 - acc: 0.8040\n",
      "Epoch 142/150\n",
      "576641/576641 [==============================] - 92s 160us/step - loss: 0.8277 - acc: 0.8047\n",
      "Epoch 143/150\n",
      "576641/576641 [==============================] - 92s 160us/step - loss: 0.8234 - acc: 0.8055\n",
      "Epoch 144/150\n",
      "576641/576641 [==============================] - 92s 159us/step - loss: 0.8186 - acc: 0.8069\n",
      "Epoch 145/150\n",
      "576641/576641 [==============================] - 92s 160us/step - loss: 0.8134 - acc: 0.8075\n",
      "Epoch 146/150\n",
      "576641/576641 [==============================] - 92s 160us/step - loss: 0.8107 - acc: 0.8075\n",
      "Epoch 147/150\n",
      "576641/576641 [==============================] - 92s 160us/step - loss: 0.8070 - acc: 0.8076\n",
      "Epoch 148/150\n",
      "576641/576641 [==============================] - 92s 160us/step - loss: 0.8021 - acc: 0.8092\n",
      "Epoch 149/150\n",
      "576641/576641 [==============================] - 92s 160us/step - loss: 0.7958 - acc: 0.8100\n",
      "Epoch 150/150\n",
      "576641/576641 [==============================] - 92s 160us/step - loss: 0.7926 - acc: 0.8105\n"
     ]
    }
   ],
   "source": [
    "# Train model on dataset\n",
    "with tf.device('/gpu:0'):\n",
    "    history = model.fit(X_reshape,\n",
    "              y_cat,\n",
    "              verbose=1,\n",
    "              batch_size=10000,\n",
    "              epochs=150, \n",
    "             )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Saving out the model for future predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "formatted_name, now, file_description= filename_format_log('../assets/LSTM315_model.h5')\n",
    "\n",
    "model.save(formatted_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "formatted_name, now, file_description= filename_format_log('../assets/LSTM315_weights.h5')\n",
    "model.save_weights(formatted_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = keras.models.load_model('../assets/1549265762_LSTM_Model.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reviewing the model summary. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_2 (Embedding)      (None, None, 3000)        40410000  \n",
      "_________________________________________________________________\n",
      "lstm_3 (LSTM)                (None, None, 300)         3961200   \n",
      "_________________________________________________________________\n",
      "lstm_4 (LSTM)                (None, 150)               270600    \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 100)               15100     \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 13470)             1360470   \n",
      "=================================================================\n",
      "Total params: 46,017,370\n",
      "Trainable params: 46,017,370\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# Print model summary\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plotting training loss alongside training accuracy (this plot was lost when the cell was mistakenly switched to markdown)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(ncols=2, nrows=1, figsize=(18,6))\n",
    "\n",
    "ax[0].plot(history.history['loss'])\n",
    "ax[0].set_title(\"Loss\", fontsize=15);\n",
    "ax[0].set_xlabel(\"epochs\",fontsize=15);\n",
    "\n",
    "ax[1].plot(history.history['acc'])\n",
    "ax[1].set_title(\"Accuracy\",fontsize=15);\n",
    "ax[1].set_xlabel(\"epochs\",fontsize=15);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generating Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample(preds, temperature=1.0):\n",
    "    preds = np.asarray(preds).astype('float64')\n",
    "    preds = np.log(preds) / temperature\n",
    "    exp_preds = np.exp(preds)\n",
    "    preds = exp_preds / np.sum(exp_preds)\n",
    "    probas = np.random.multinomial(1, preds, 1)\n",
    "    return np.argmax(probas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_lyrics(seed, \n",
    "                    seq_len = 4, \n",
    "                    song_len = 50,\n",
    "                    temperature = 1.0,\n",
    "                    model = None,\n",
    "                    tokenizer = None,\n",
    "                    model_dir = None, \n",
    "                    tokenizer_dir = None):\n",
    "    \n",
    "    if model_dir:\n",
    "        model = keras.models.load_model('../assets/1549265762_LSTM_Model.h5')\n",
    "    else:\n",
    "        model = model\n",
    "    \n",
    "    if tokenizer_dir: \n",
    "        with open('../assets/1549307085_190204_tokenizer.pkl', 'rb') as f:\n",
    "            tokenizer = pickle.load(f)\n",
    "    else:\n",
    "        tokenizer = tokenizer\n",
    "    \n",
    "    seed_clean = seed.lower().split(' ')\n",
    "    doc = []\n",
    "\n",
    "    while len(doc) < song_len:\n",
    "        text = [seed_clean]\n",
    "        sequence = [tokenizer.texts_to_sequences([word])[0] for word in text]\n",
    "        pad_sequence = pad_sequences(sequence, maxlen=seq_len, truncating='pre')\n",
    "        sequence_reshape = np.reshape(pad_sequence, (1, seq_len))\n",
    "\n",
    "        yhat = model.predict(sequence_reshape, verbose=0)[0]\n",
    "        next_index = sample(yhat, temperature)\n",
    "        \n",
    "        for word, index in tokenizer.word_index.items():\n",
    "            if index == next_index:\n",
    "                seed_clean.append(word)\n",
    "                doc.append(word)\n",
    "\n",
    "    return ' '.join(doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/ipykernel_launcher.py:3: RuntimeWarning: divide by zero encountered in log\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n"
     ]
    }
   ],
   "source": [
    "lyrics = generate_lyrics('eyes are for lovers',\n",
    "                         seq_len=4,\n",
    "                         song_len=250,\n",
    "                         model=model,\n",
    "                         tokenizer=tokenizer\n",
    "                        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " you can cut the fake shit \n",
      " im not that smile with your number \n",
      " seventeen and under maybe just a little younger \n",
      " had to get out oh oh and wont you come inside \n",
      " ill take you in the room suga \n",
      " lock you up and love but held me up to your heart of joy and times together \n",
      " now that certain things be and a running by now yet just wait oh betcha and one one lady get it \n",
      " tonight is real love isnt this piece of control and shes standing on the edge of a waking dream \n",
      " over rivers \n",
      " over streams \n",
      " through wind and rain \n",
      " ill be ok \n",
      " dont let me go \n",
      " dont let me slip quick with her \n",
      " cos youre my best friend \n",
      " would not be a pack above \n",
      " put your air from her years \n",
      " because your days are here and i know \n",
      " oh darling of mine please dont let go \n",
      " dont you wanna lose you \n",
      " i think i wouldnt change you so \n",
      " could go my line \n",
      " but we cant do \n",
      " and if i ever fail you darling \n",
      " let me know if its further \n",
      " all i just want to see it through \n",
      " and somewhere in my heart \n",
      " but just like you \n",
      " cause beautiful love ima last forever \n",
      " nah of forever \n",
      " so please dont go \n",
      " love it once i\n"
     ]
    }
   ],
   "source": [
    "print(lyrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/ipykernel_launcher.py:3: RuntimeWarning: divide by zero encountered in log\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n"
     ]
    }
   ],
   "source": [
    "lyrics = generate_lyrics('eyes are for lovers',\n",
    "                         seq_len=4,\n",
    "                         song_len=250,\n",
    "                         model=model,\n",
    "                         tokenizer=tokenizer\n",
    "                        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " what im thinking about what you said \n",
      " just me and you ooh for you \n",
      " all i ever needed was some closure \n",
      " come closer oh oh yes make me cause ha mmmmmm yeah \n",
      " sight twenty gold your clothes niggas in check i saw you yeah \n",
      " you dont have to be so exciting \n",
      " just trying to give them a run of sky \n",
      " no two sususussudio oh oh \n",
      " now im bringin up the past shit oh oh \n",
      " oh oh \n",
      " and if you stay beside me \n",
      " as long as im with you i wanna be with you yeah \n",
      " ill be waiting right here just to eat it was ya \n",
      " make it slow take your time on me \n",
      " said hold on cant stop la yeah yeah ooh \n",
      " oohoohoohooh \n",
      " love me love me \n",
      " mm thats right right there \n",
      " just to be thankful \n",
      " that i had feeling falling \n",
      " and that well find how \n",
      " when we found out \n",
      " if you get lonely \n",
      " theres not somebody i want you to stay so \n",
      " dont go dancing in my minds where the broken thought and when that with those blind of tipping in wedding compartment temor \n",
      " iu becomes pretty pretty \n",
      " life this road we gave me this \n",
      " a little gold goin through around im careful oh \n",
      " ooh baby oh listen to oh \n",
      " lovin fun rains and that together have to buy\n"
     ]
    }
   ],
   "source": [
    "print(lyrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/ipykernel_launcher.py:3: RuntimeWarning: divide by zero encountered in log\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n"
     ]
    }
   ],
   "source": [
    "lyrics = generate_lyrics('eyes are for lovers',\n",
    "                         seq_len=4,\n",
    "                         song_len=250,\n",
    "                         model=model,\n",
    "                         tokenizer=tokenizer\n",
    "                        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " you can take your time \n",
      " whatever youre resting me \n",
      " and when i feel your love \n",
      " promiscuous girl shes wondering it \n",
      " enough \n",
      " i know its meant to be itll be itll be \n",
      " baby just let it be im alright \n",
      " dont nobody worry bout me \n",
      " you got me tempted to say fuck it \n",
      " oh im ready oh dont be good \n",
      " ooo oh you love me be with \n",
      " i wish a little thing seems to hold me with them \n",
      " i wanna be a respond thing \n",
      " but neither i know \n",
      " sometimes its easy to hear \n",
      " if youre gonna live without me \n",
      " oh \n",
      " why you make it so complicated \n",
      " off the drink we concentratin \n",
      " i know you want it \n",
      " theres people cause every day our day and i wont fall in love without you \n",
      " i cant go on \n",
      " i wanna be shots \n",
      " cause everybody comes and get through the wall to sleep over \n",
      " cause baby you are stuck stuck \n",
      " you are stuck with me \n",
      " said i know \n",
      " and i know god will open into your heart \n",
      " well ima lay it in love me i wont go \n",
      " just want you to stay \n",
      " i want you to know that he are thinking youre on my mind \n",
      " am i gone deep to see us \n",
      " i i wish that time would find it \n",
      " word for\n"
     ]
    }
   ],
   "source": [
    "print(lyrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/ipykernel_launcher.py:3: RuntimeWarning: divide by zero encountered in log\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " why told you i want you \n",
      " oh nobody til im happy \n",
      " ill do what you want \n",
      " a two two smoking you gotta show up lookin so good just to show you right \n",
      " just too good to say \n",
      " but i took this on the summer \n",
      " let me inside you baby \n",
      " let me chill \n",
      " dont cha wish your girlfriend was raw like me raw \n",
      " dont cha wish your girlfriend was hot like me \n",
      " am i crazy \n",
      " for a second for a minute can we go \n",
      " if we made everything will just know when \n",
      " i wonder who i saw \n",
      " i work for the trash \n",
      " came up its just me that they just wanted to do go \n",
      " are you here to \n",
      " only on weekends \n",
      " only shot will bleed rain \n",
      " you say it best \n",
      " im already your one true weakness \n",
      " so let me go down down down down \n",
      " running out of things that it aint too \n",
      " on this record \n",
      " man leaving knockin tryin with your night \n",
      " so i walk in the spot this is what i thought \n",
      " thinking that i just cant get enough \n",
      " i cant get you off my mind \n",
      " but you can go right now the very shore \n",
      " oh dont try to live so good \n",
      " im going back to the start \n",
      " that pull you picture me after it not coming home\n"
     ]
    }
   ],
   "source": [
    "lyrics = generate_lyrics('you break my heart',\n",
    "                         seq_len=4,\n",
    "                         song_len=250,\n",
    "                         model=model,\n",
    "                         tokenizer=tokenizer\n",
    "                        )\n",
    "\n",
    "print(lyrics)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
