{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notebook 03: Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ! pip install psycopg2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.2.4\n"
     ]
    }
   ],
   "source": [
    "import keras\n",
    "print(keras.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.12.0\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json, time, re, string, keras, adanet, pickle\n",
    "import pandas as pd\n",
    "import psycopg2 as pg2\n",
    "import numpy as np\n",
    "\n",
    "from numpy import random\n",
    "from psycopg2.extras import RealDictCursor, Json\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Embedding, LSTM\n",
    "from keras.utils.np_utils import to_categorical\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline\n",
    "%run ../assets/sql_cred.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filename_format_log(file_path, \n",
    "                        logfile = '../assets/file_log.txt', \n",
    "                        now = round(time.time()), \n",
    "                        file_description = None): \n",
    "   \n",
    "    try:\n",
    "        ext = re.search('(?<!^)(?<!\\.)\\.(?!\\.)', file_path).start() \n",
    "    except:\n",
    "        raise NameError('Please enter a relative path with a file extension.') \n",
    "    \n",
    "    stamp = re.search('(?<!^)(?<!\\.)[0-z]+_[0-z]+(?=\\.)', file_path).start()\n",
    "    formatted_name = f'{file_path[:stamp]}{now}_{file_path[stamp:]}'  \n",
    "    if not file_description:\n",
    "        file_description = f'Saved at: {time.asctime(time.gmtime(now))}'\n",
    "    with open(logfile, 'a+') as f:\n",
    "        f.write(f'{formatted_name}: {file_description}\\n')\n",
    "    return formatted_name, now, file_description"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "def con_cur_to_db(dbname=DBNAME, dict_cur=None):\n",
    "    con = pg2.connect(host=IP_ADDRESS,\n",
    "                  dbname=dbname,\n",
    "                  user=USER,\n",
    "                  password=PASSWORD)\n",
    "    if dict_cur:\n",
    "        cur = con.cursor(cursor_factory=RealDictCursor)\n",
    "    else:\n",
    "        cur = con.cursor()\n",
    "    return con, cur\n",
    "    \n",
    "def execute_query(query, dbname=DBNAME, dict_cur=None, command=False):\n",
    "    con, cur = con_cur_to_db(dbname, dict_cur)\n",
    "    cur.execute(f'{query}')\n",
    "    if not command:\n",
    "        data = cur.fetchall()\n",
    "        con.close()\n",
    "        return data\n",
    "    con.commit() #sends to server\n",
    "    con.close() #closes server connection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "lyric_df = pd.read_csv('../assets/1549341381_lyric_df.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>lyrics</th>\n",
       "      <th>clean_lyrics</th>\n",
       "      <th>total_words_track</th>\n",
       "      <th>unique_words_track</th>\n",
       "      <th>total_lines_track</th>\n",
       "      <th>unique_lines_track</th>\n",
       "      <th>mean_words_line</th>\n",
       "      <th>mean_unique_words_line</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>\\n\\nIf your needle is near\\nNeedle is near\\nYo...</td>\n",
       "      <td>if your needle is near \\n needle is near \\n yo...</td>\n",
       "      <td>57</td>\n",
       "      <td>20</td>\n",
       "      <td>14</td>\n",
       "      <td>8</td>\n",
       "      <td>5.9</td>\n",
       "      <td>5.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>\\n\\n[Verse 1]\\nBrown skin girl on the other si...</td>\n",
       "      <td>brown skin girl on the other side of the room ...</td>\n",
       "      <td>132</td>\n",
       "      <td>52</td>\n",
       "      <td>24</td>\n",
       "      <td>13</td>\n",
       "      <td>7.4</td>\n",
       "      <td>5.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>\\n\\n[Verse 1]\\nIt's simple, I love it\\nHaving ...</td>\n",
       "      <td>its simple i love it \\n having you near me hav...</td>\n",
       "      <td>151</td>\n",
       "      <td>63</td>\n",
       "      <td>29</td>\n",
       "      <td>21</td>\n",
       "      <td>7.1</td>\n",
       "      <td>5.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>\\n\\n[Intro: Whistling]\\n\\n[Verse 1]\\nA great b...</td>\n",
       "      <td>a great big bang and dinosaurs \\n fiery rainin...</td>\n",
       "      <td>126</td>\n",
       "      <td>76</td>\n",
       "      <td>20</td>\n",
       "      <td>18</td>\n",
       "      <td>8.2</td>\n",
       "      <td>7.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>\\n\\n[Verse 1]\\nIsn't she lovely\\nIsn't she won...</td>\n",
       "      <td>isnt she lovely \\n isnt she wonderful \\n isnt ...</td>\n",
       "      <td>108</td>\n",
       "      <td>66</td>\n",
       "      <td>21</td>\n",
       "      <td>20</td>\n",
       "      <td>7.0</td>\n",
       "      <td>6.1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              lyrics  \\\n",
       "0  \\n\\nIf your needle is near\\nNeedle is near\\nYo...   \n",
       "1  \\n\\n[Verse 1]\\nBrown skin girl on the other si...   \n",
       "2  \\n\\n[Verse 1]\\nIt's simple, I love it\\nHaving ...   \n",
       "3  \\n\\n[Intro: Whistling]\\n\\n[Verse 1]\\nA great b...   \n",
       "4  \\n\\n[Verse 1]\\nIsn't she lovely\\nIsn't she won...   \n",
       "\n",
       "                                        clean_lyrics  total_words_track  \\\n",
       "0  if your needle is near \\n needle is near \\n yo...                 57   \n",
       "1  brown skin girl on the other side of the room ...                132   \n",
       "2  its simple i love it \\n having you near me hav...                151   \n",
       "3  a great big bang and dinosaurs \\n fiery rainin...                126   \n",
       "4  isnt she lovely \\n isnt she wonderful \\n isnt ...                108   \n",
       "\n",
       "   unique_words_track  total_lines_track  unique_lines_track  mean_words_line  \\\n",
       "0                  20                 14                   8              5.9   \n",
       "1                  52                 24                  13              7.4   \n",
       "2                  63                 29                  21              7.1   \n",
       "3                  76                 20                  18              8.2   \n",
       "4                  66                 21                  20              7.0   \n",
       "\n",
       "   mean_unique_words_line  \n",
       "0                     5.1  \n",
       "1                     5.8  \n",
       "2                     5.8  \n",
       "3                     7.2  \n",
       "4                     6.1  "
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lyric_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>total_words_track</th>\n",
       "      <th>unique_words_track</th>\n",
       "      <th>total_lines_track</th>\n",
       "      <th>unique_lines_track</th>\n",
       "      <th>mean_words_line</th>\n",
       "      <th>mean_unique_words_line</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>1785.000000</td>\n",
       "      <td>1785.000000</td>\n",
       "      <td>1785.000000</td>\n",
       "      <td>1785.000000</td>\n",
       "      <td>1785.000000</td>\n",
       "      <td>1785.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>284.514286</td>\n",
       "      <td>96.840896</td>\n",
       "      <td>40.661064</td>\n",
       "      <td>27.560224</td>\n",
       "      <td>11.542241</td>\n",
       "      <td>8.510756</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>136.291119</td>\n",
       "      <td>44.114334</td>\n",
       "      <td>18.013478</td>\n",
       "      <td>12.201590</td>\n",
       "      <td>62.028609</td>\n",
       "      <td>24.885099</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>13.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>4.200000</td>\n",
       "      <td>3.600000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>195.000000</td>\n",
       "      <td>74.000000</td>\n",
       "      <td>28.000000</td>\n",
       "      <td>20.000000</td>\n",
       "      <td>7.900000</td>\n",
       "      <td>6.600000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>267.000000</td>\n",
       "      <td>92.000000</td>\n",
       "      <td>38.000000</td>\n",
       "      <td>25.000000</td>\n",
       "      <td>8.800000</td>\n",
       "      <td>7.400000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>346.000000</td>\n",
       "      <td>112.000000</td>\n",
       "      <td>51.000000</td>\n",
       "      <td>33.000000</td>\n",
       "      <td>9.900000</td>\n",
       "      <td>8.300000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>2230.000000</td>\n",
       "      <td>956.000000</td>\n",
       "      <td>224.000000</td>\n",
       "      <td>189.000000</td>\n",
       "      <td>2230.000000</td>\n",
       "      <td>956.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       total_words_track  unique_words_track  total_lines_track  \\\n",
       "count        1785.000000         1785.000000        1785.000000   \n",
       "mean          284.514286           96.840896          40.661064   \n",
       "std           136.291119           44.114334          18.013478   \n",
       "min            13.000000            5.000000           1.000000   \n",
       "25%           195.000000           74.000000          28.000000   \n",
       "50%           267.000000           92.000000          38.000000   \n",
       "75%           346.000000          112.000000          51.000000   \n",
       "max          2230.000000          956.000000         224.000000   \n",
       "\n",
       "       unique_lines_track  mean_words_line  mean_unique_words_line  \n",
       "count         1785.000000      1785.000000             1785.000000  \n",
       "mean            27.560224        11.542241                8.510756  \n",
       "std             12.201590        62.028609               24.885099  \n",
       "min              1.000000         4.200000                3.600000  \n",
       "25%             20.000000         7.900000                6.600000  \n",
       "50%             25.000000         8.800000                7.400000  \n",
       "75%             33.000000         9.900000                8.300000  \n",
       "max            189.000000      2230.000000              956.000000  "
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lyric_df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_sequence(text, sequence_length = 7, output_length = 4):\n",
    "    \n",
    "    X, y = [], []\n",
    "    \n",
    "    split_text = re.split('(\\n)|(\\[.+\\])|\\s', text)\n",
    "    split_text = list(filter(None, split_text))\n",
    "    split_text = text\n",
    "    \n",
    "    for i in range(len(split_text) - sequence_length):\n",
    "        X.append(split_text[i:i + sequence_length])\n",
    "        y.append(split_text[i + sequence_length:i + sequence_length + output_length])\n",
    "        \n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_samples(X_indexed, y_indexed):\n",
    "    partition = {}\n",
    "    labels = {}\n",
    "\n",
    "    random_seed = 42\n",
    "    rand_ind = random.choice(range(len(X_indexed)), len(X_indexed), replace=False)\n",
    "    \n",
    "    partition['train'] = rand_ind[:int(np.ceil(len(rand_ind)*.8))]\n",
    "    partition['validation'] = rand_ind[-int(np.ceil(len(rand_ind)*.2))+1:]\n",
    "\n",
    "    for i in rand_ind:\n",
    "        labels[i] = y_indexed[i]\n",
    "        \n",
    "    return partition, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_lyrics(df, lyrics_col, seq_len, output_len, save_dir='../assets'):\n",
    "    X = []\n",
    "    y = []\n",
    "    \n",
    "    corpus = []\n",
    "    \n",
    "    print('Processing lyrics...')\n",
    "    for _, track in df[lyrics_col].iterrows():\n",
    "        lyrics = track[0]\n",
    "        lyrics_spaced = re.sub(r'( +)', ' ', lyrics)\n",
    "        lyrics_split = lyrics_spaced.split(' ')\n",
    "        corpus.extend(lyrics_split)\n",
    "                \n",
    "        for i in range(len(lyrics_split) - seq_len):\n",
    "            X.append(np.array(lyrics_split[i:i + seq_len]))\n",
    "            y.extend(np.array(lyrics_split[i + seq_len:i + seq_len + output_len]))\n",
    "            \n",
    "    print('Fitting Tokenizer...')\n",
    "    tokenizer = Tokenizer(oov_token=0)\n",
    "    tokenizer.filters = tokenizer.filters.replace('\\n', '')\n",
    "    tokenizer.fit_on_texts(corpus)\n",
    "\n",
    "    vocab_size = len(tokenizer.word_index) + 1\n",
    "        \n",
    "    print(f'Vocab size = {vocab_size}')\n",
    "#         formatted_name, now, file_description= filename_format_log(f'{save_dir}/tokenizer.pkl')\n",
    "\n",
    "#         with open(formatted_name, 'wb+') as f:\n",
    "#             pickle.dump(tokenizer, f)\n",
    "#         print(f'Tokenizer saved to {formatted_name}.')          \n",
    "\n",
    "    print('Indexing sequences...')\n",
    "    X_indexed = [[tokenizer.texts_to_sequences([word])[0] for word in row] for row in X]\n",
    "    y_indexed = [tokenizer.texts_to_sequences([word])[0] for word in y]\n",
    "    \n",
    "    print('Partitioning and converting to labels...')\n",
    "\n",
    "    partition, labels = generate_samples(X_indexed, y_indexed)\n",
    "    \n",
    "    np.save(f'{save_dir}/data.npy', partition)\n",
    "#     X_reshape = np.reshape(X_indexed, (len(X_indexed), seq_len, 1))\n",
    " \n",
    "#     y_cat = to_categorical(y_indexed)\n",
    "    \n",
    "    print('Lyrics successfully tokenized, sequenced, and indexed.') \n",
    "    \n",
    "    return partition, labels, vocab_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing lyrics...\n",
      "Fitting Tokenizer...\n",
      "Vocab size = 13470\n",
      "Indexing sequences...\n",
      "Partitioning and converting to labels...\n",
      "Lyrics successfully tokenized, sequenced, and indexed.\n"
     ]
    }
   ],
   "source": [
    "partition, labels, vocab_size = tokenize_lyrics(df=lyric_df,\n",
    "                                   lyrics_col=['clean_text'],\n",
    "                                   seq_len=4,\n",
    "                                   output_len=1,\n",
    "                                   save_dir='../assets'\n",
    "                                  )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'train': array([356649, 109240,   2957, ...,  87503, 436021,  78552]),\n",
       " 'validation': array([435192, 493343, 395344, ..., 133438, 191157, 320662])}"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "partition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "not_int = []\n",
    "for i, part in enumerate(partition['train']):\n",
    "    if not np.issubdtype(partition['train'][0],np.int64):\n",
    "        not_int.append(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataGenerator(keras.utils.Sequence):\n",
    "    'Generates data for Keras'\n",
    "    def __init__(self, list_IDs, labels, batch_size=5000, dim=(5000,1,4), n_channels=1,\n",
    "                 n_classes=10, shuffle=True):\n",
    "        'Initialization'\n",
    "        self.dim = dim\n",
    "        self.batch_size = batch_size\n",
    "        self.labels = labels\n",
    "        self.list_IDs = list_IDs\n",
    "        self.n_channels = n_channels\n",
    "        self.n_classes = n_classes\n",
    "        self.shuffle = shuffle\n",
    "        self.on_epoch_end()\n",
    "\n",
    "    def __len__(self):\n",
    "        'Denotes the number of batches per epoch'\n",
    "        return int(np.floor(len(self.list_IDs) / self.batch_size))\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        'Generate one batch of data'\n",
    "        # Generate indexes of the batch\n",
    "        indexes = self.indexes[index*self.batch_size:(index+1)*self.batch_size]\n",
    "\n",
    "        # Find list of IDs\n",
    "        list_IDs_temp = [self.list_IDs[k] for k in indexes]\n",
    "\n",
    "        # Generate data\n",
    "        X, y = self.__data_generation(list_IDs_temp)\n",
    "\n",
    "        return X, y\n",
    "\n",
    "    def on_epoch_end(self):\n",
    "        'Updates indexes after each epoch'\n",
    "        self.indexes = np.arange(len(self.list_IDs))\n",
    "        if self.shuffle == True:\n",
    "            np.random.shuffle(self.indexes)\n",
    "\n",
    "    def __data_generation(self, list_IDs_temp):\n",
    "        'Generates data containing batch_size samples' # X : (n_samples, *dim, n_channels)\n",
    "        # Initialization\n",
    "        X = np.empty((self.batch_size, *self.dim, self.n_channels))\n",
    "        y = np.empty((self.batch_size), dtype=int)\n",
    "\n",
    "        # Generate data\n",
    "        for i, ID in enumerate(list_IDs_temp):\n",
    "            # Store sample\n",
    "            X[i] = self.list_IDs[ID]\n",
    "\n",
    "            # Store class\n",
    "            y[i] = self.labels[ID]\n",
    "\n",
    "        return X, keras.utils.to_categorical(y, num_classes=self.n_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters\n",
    "params = {'dim': (5000,1,4),\n",
    "          'batch_size': 5000,\n",
    "          'n_classes': vocab_size,\n",
    "          'n_channels': 1,\n",
    "          'shuffle': True\n",
    "         }\n",
    "\n",
    "# Datasets\n",
    "partition = partition\n",
    "labels = labels\n",
    "\n",
    "# Generators\n",
    "training_generator = DataGenerator(partition['train'], labels, **params)\n",
    "validation_generator = DataGenerator(partition['validation'], labels, **params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lyric_df['cleaner_text'] = lyric_df['clean_text'].map(lambda x: re.sub(r'( +)', ' ', x).split(' '))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[corpus.add(word) for song in lyric_df['clean_text'].map(lambda x: re.sub(r'( +)', ' ', x).split(' ')).values for word in song];"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# seq_len, output_len = 4, 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def sequencer(lyrics_split, seq_len=4, output_len=1):\n",
    "#     X, y = [], []\n",
    "#     for i in range(len(lyrics_split) - seq_len):\n",
    "#         X.append(lyrics_split[i:i + seq_len])\n",
    "#         y.extend(lyrics_split[i + seq_len:i + seq_len + output_len])\n",
    "#     return X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# foo = pd.DataFrame([1,2,3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# result = lyric_df.cleaner_text.map(sequencer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# result[0][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lyrics_split[i + 4:i + 4 + 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample(yhat, temperature=1.0):\n",
    "    yhats = np.asarray(yhat).astype('float64')\n",
    "    yhat = np.log(yhat) / temperature\n",
    "    exp_preds = np.exp(yhat)\n",
    "    yhat = exp_preds / np.sum(exp_preds)\n",
    "    probas = np.random.multinomial(1, yhat, 1)\n",
    "    return np.argmax(probas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def generate_lyrics(self,\n",
    "seed = 'This is an example seed'\n",
    "seq_len = 4\n",
    "song_len = 50\n",
    "temperature = 1.0\n",
    "\n",
    "seed_clean = [seed.lower().split(' ')]\n",
    "doc = seed_clean\n",
    "\n",
    "seed_len = len(seed_clean[0])\n",
    "num_seq = int(np.ceil(seed_len/seq_len))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [],
   "source": [
    "# while len(doc) < np.abs((song_len-seed_len)):\n",
    "text = seed_clean\n",
    "sequence = [tokenizer.texts_to_sequences([word])[0] for word in text]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[35, 30, 184, 10668, 2806]]"
      ]
     },
     "execution_count": 160,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 161,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_seq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [],
   "source": [
    "if seed_len % seq_len != 0:\n",
    "    pad = (num_seq*seq_len) - seed_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [],
   "source": [
    "sequence[0].extend(np.zeros(pad, dtype='int'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[35, 30, 184, 10668, 2806, 0, 0, 0]]"
      ]
     },
     "execution_count": 164,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [],
   "source": [
    "sequence = np.reshape(sequence[0], (num_seq, seq_len))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[   35,    30,   184, 10668],\n",
       "       [ 2806,     0,     0,     0]])"
      ]
     },
     "execution_count": 166,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pad_sequence = pad_sequences(sequence, maxlen=seq_len, truncating='pre')\n",
    "sequence_reshape = np.reshape(pad_sequence, (1, seq_len))\n",
    "\n",
    "yhat = self.model.predict(sequence_reshape, verbose=0)[0]\n",
    "next_index = self.sample(yhat, temperature)\n",
    "\n",
    "for word, index in tokenizer.word_index.items():\n",
    "    if index == next_index:\n",
    "        seed_clean.append(word)\n",
    "        doc.append(word)\n",
    "\n",
    "self.output = ' '.join(doc)\n",
    "print(self.output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def track_structure(track):\n",
    "    words_track = []    \n",
    "    lines_track = []\n",
    "    \n",
    "    lyrics_spaced = re.sub(r'( +)', ' ', track)\n",
    "    \n",
    "    lyrics_split = lyrics_spaced.split(' ')\n",
    "    \n",
    "    lines_split = lyrics_spaced.split('\\n')\n",
    "    lines_track.append(lines_split)\n",
    "    \n",
    "    no_nl = re.sub(r'\\n ', '', lyrics_spaced)\n",
    "    lyrics_split = no_nl.split(' ')\n",
    "    words_track.append(lyrics_split)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print('Creating encoding dicts from corpus...')\n",
    "# words = sorted(list(set(corpus)))\n",
    "# print(f'Count of unique words (i.e., features): {len(words)}')\n",
    "# words_index = dict((c, i+1) for i, c in enumerate(words))\n",
    "# index_words = dict((i+1, c) for i, c in enumerate(words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def split_sequence(text, sequence_length = 7, output_length = 4):\n",
    "    \n",
    "#     X, y = [], []\n",
    "    \n",
    "#     split_text = re.split('(\\n)|(\\[.+\\])|\\s', text)\n",
    "#     split_text = list(filter(None, split_text))\n",
    "#     split_text = text\n",
    "    \n",
    "#     for i in range(len(split_text) - sequence_length):\n",
    "#         X.append(split_text[i:i + sequence_length])\n",
    "#         y.append(split_text[i + sequence_length:i + sequence_length + output_length])\n",
    "        \n",
    "#     return X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print('Indexing sequences...')\n",
    "# X_indexed = [[words_index[word] for word in row] for row in X]\n",
    "# y_indexed = [words_index[word] for word in y]\n",
    "# print('Number of sequences')\n",
    "# print('Partitioning and converting to labels...')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#     partition, labels = generate_samples(X_indexed, y_indexed)\n",
    "\n",
    "#     np.save(f'{save_dir}/data.npy', partition)\n",
    "# X_reshape = np.reshape(X_indexed, (len(X_indexed), seq_len))\n",
    "\n",
    "# y_cat = to_categorical(y_indexed)\n",
    "\n",
    "# print('Lyrics successfully tokenized, sequenced, and indexed.') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def generate_samples(X_indexed, y_cat, seq_len=4, random_seed = 42):\n",
    "#     X_train = [] \n",
    "#     X_test = []\n",
    "#     y_train = []\n",
    "#     y_test = []\n",
    "    \n",
    "#     rand_ind = random.choice(range(len(X_indexed)), len(X_indexed), replace=False)\n",
    "    \n",
    "#     train_ind = rand_ind[:int(np.ceil(len(rand_ind)*.8))]\n",
    "#     test_ind = rand_ind[-int(np.ceil(len(rand_ind)*.2))+1:]\n",
    "\n",
    "#     for i in train_ind:\n",
    "#         X_train.append(X_indexed[i])\n",
    "#         y_train.append(y_cat[i])\n",
    "        \n",
    "#     for i in test_ind:\n",
    "#         X_test.append(X_indexed[i])\n",
    "#         y_test.append(y_cat[i])\n",
    "        \n",
    "#     return np.reshape(X_train, (len(X_train), seq_len)) , np.reshape(X_test, (len(X_test), seq_len)), np.array(y_train), np.array(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_train, X_test, y_train, y_test = generate_samples(X_indexed, y_cat)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
